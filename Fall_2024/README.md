# gpLM-reading-group

This repo contains the "curriculum" and resources for a genomic and protein language model reading group at University of Colorado Boulder/Anschutz for Fall 2024.

## Topics
Topics will focus on architecture and applications of genomic and protein language models. 

## Papers/curriculum/schedule

**THIS IS A LIVE AND ROUGH DRAFT - PLEASE FEEL FREE TO OPEN AN ISSUE OR SUBMIT A PULL REQUEST WITH SUGGESTIONS OR MORE DETAILS**. Alternatively, you can email me at john dot sterrett @colorado.edu.

1. October 7 (4:30 pm with food provided by BioFrontiers QED)
    1. Survey of Genomic and Protein Language Models and their applications
        1. [Genomic language models: opportunities and challenges](https://arxiv.org/html/2407.11435v1)

2. October 21 (10-11 am)
    1. Intro to language models and architecture - 3 video lectures as a starting point for discussion
    2. Material:
        1. [How large language models work, a visual intro to transformers](https://youtu.be/wjZofJX0v4M)
        2. [Attention in transformers, visually explained](https://youtu.be/eMlx5fFNoYc)
        3. [How might LLMs store facts](https://youtu.be/9-Jl0dxWQs8)

3. November 4 (10-11 am)
    1. Early applications of transformer-based models to biology
    2. BERT
        1. [Original paper](https://arxiv.org/abs/1810.04805)
        2. [BERTology meets Biology](https://arxiv.org/abs/2006.15222)
        3. [DNABERT](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)
    3. Material:
        1. Brief video overview of BERT ([here](https://youtu.be/t45S_MwAcOw?t=340)).
           - Start around 5:40, unless you would like a recap on transformers, then start from the beginning. The video is ~12 min long, so starting at the recommended point makes it 6 min.
        2. Main paper for this week will be “[BERTology Meets Biology: Interpreting Attention in Protein Language Models](https://arxiv.org/abs/2006.15222)”.
           - If the paper feels daunting, you want a better explanation of the paper, or if you’re just tired of reading, there’s a nice video explaining this paper! It can be found [here](https://youtu.be/q6Kyvy1zLwQ) and is ~37 minutes, but if you have a solid biology background, you can skip through the 10-12 minutes.
        3. We’ll also try to discuss some results from the original DNABERT paper ([here](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680), so if you have spare time please read this.
           - Note that DNABERT2 has been released, but I think reading the original paper would be helpful because it provides a little more background and gets into more biological benchmarks. The DNABERT2 paper mostly focuses on improvements to the original model.
 

4. November 18 (10-11 am)
    1. Protein structure and engineering
        1. [AlphaFold3](https://www.nature.com/articles/s41586-024-07487-w)?
        2. [PoET](https://www.openprotein.ai/poet-a-high-performing-protein-language-model-for-zero-shot-prediction)?
        
5. December 2 (10-11 am)
    1. Metagenomics 
        1. [Learning coevolutionary signals with the first mixed-modality genomic language model and the OMG corpus](https://www.biorxiv.org/content/10.1101/2024.08.14.607850v1)
        2. [Protein language models uncover CAZyme function in metagenomics](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10634757/)
        3. Potentially have Kumar come talk about the project and revelant considerations?
    2. Learning gene/protein regulation
        1. [Genomic language model predicts protein co-regulation and function](https://www.nature.com/articles/s41467-024-46947-9)

7. Learning evolutionary dynamics?
8. Hands-on project?
